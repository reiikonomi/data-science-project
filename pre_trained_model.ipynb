{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded images shape: (969, 224, 224, 3)\n",
      "Loaded labels shape: (969,)\n"
     ]
    }
   ],
   "source": [
    "DATADIR = \"dataset/train/\"\n",
    "CATEGORIES = [\"lajthiza\", \"spring\", \"tepelena\"]\n",
    "\n",
    "# Defining the size of the images expected by MobileNetV2\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Loading images directly from the dataset folder and preprocess them\n",
    "def load_images():\n",
    "    X = []\n",
    "    y = []\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR, category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            if img.endswith(\".jpg\"):  # Ensure only .jpg files are processed\n",
    "                try:\n",
    "                    img_path = os.path.join(path, img)\n",
    "                    image = load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "                    image = img_to_array(image)\n",
    "                    image = preprocess_input(image)\n",
    "                    X.append(image)\n",
    "                    y.append(class_num)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image {img_path}: {e}\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Loading the images and labels\n",
    "X, y = load_images()\n",
    "\n",
    "# Verifying the shape of the loaded data\n",
    "print(f\"Loaded images shape: {X.shape}\")\n",
    "print(f\"Loaded labels shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (775, 224, 224, 3)\n",
      "y_train shape: (775, 3)\n",
      "X_val shape: (194, 224, 224, 3)\n",
      "y_val shape: (194, 3)\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(np.unique(y))  # Automatically determine number of classes\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=num_classes)\n",
    "\n",
    "# Spliting the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verifying data shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "# Freezing the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 720ms/step - accuracy: 0.6574 - loss: 0.8425 - val_accuracy: 1.0000 - val_loss: 0.0281\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 695ms/step - accuracy: 0.9874 - loss: 0.0538 - val_accuracy: 0.9948 - val_loss: 0.0225\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 838ms/step - accuracy: 0.9858 - loss: 0.0335 - val_accuracy: 1.0000 - val_loss: 0.0049\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 681ms/step - accuracy: 1.0000 - loss: 0.0122 - val_accuracy: 1.0000 - val_loss: 0.0033\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 628ms/step - accuracy: 0.9979 - loss: 0.0094 - val_accuracy: 1.0000 - val_loss: 0.0023\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 628ms/step - accuracy: 0.9998 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0024\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 607ms/step - accuracy: 0.9976 - loss: 0.0106 - val_accuracy: 1.0000 - val_loss: 0.0018\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 584ms/step - accuracy: 1.0000 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 575ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0010\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 612ms/step - accuracy: 1.0000 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 9.0330e-04\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 409ms/step - accuracy: 1.0000 - loss: 7.3336e-04\n",
      "Validation accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluating the model\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Validation accuracy: {val_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('water_bottle_classifier_mobilenetv2_pretrained.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
